KSQL Task Steps

cd Desktop/Platformatory/Daily\ Codes/ksql-tasks/ksql-exercises/
docker-compose up -d --build
docker-compose up -d

Execute the following container command
docker-compose exec postgres bash

Once inside the container, execute the following command.
psql -U platformatory -d plf_training

List all tables
\dt

View contents of a particular tables
SELECT * FROM product;

To exit from the tables
\q

sudo docker exec -u root -it kafka-1 /bin/bash
docker-compose exec ksqldb-cli  ksql http://ksqldb-server:8088
lsof -i :5432
sudo netstat -tulnp | grep 5432
sudo systemctl status postgresql
sudo systemctl stop postgresql
sudo systemctl status postgresql
sudo netstat -tulnp | grep 5432
DROP STREAM PURCHASE_DETAILS;


kafka-topics --bootstrap-server kafka-1:9091 --topic purchase --create --partitions 6 --replication-factor 3
kafka-topics --bootstrap-server kafka-1:9091 --topic product --create --partitions 3 --replication-factor 3
kafka-topics --bootstrap-server kafka-1:9091 --list
kafka-console-consumer --bootstrap-server kafka-1:9091 --topic purchase --from-beginning --property print.key=true
kafka-console-consumer --bootstrap-server kafka-1:9091 --topic product --from-beginning --property print.key=true

curl -X DELETE http://connect:8083/connectors/datagen-purchase
curl http://connect:8083/connectors/jdbc_source_postgres/status
curl http://connect:8083/connectors/datagen-purchase/status


curl -X POST \
  -H "Content-Type: application/json" \
  --data @/kafka/datagen-purchase-config.json \
  http://connect:8083/connectors

{"name":"datagen-purchase","config":{"connector.class":"io.confluent.kafka.connect.datagen.DatagenConnector","kafka.topic":"purchase","value.schema":"$(cat /kafka/purchase-schema.json)","schema.filename":"/kafka/purchase-schema.json","max.interval":"1000","iterations":"-1","tasks.max":"1","key.converter":"org.apache.kafka.connect.storage.StringConverter","value.converter":"io.confluent.connect.avro.AvroConverter","value.converter.schema.registry.url":"http://schema-registry:8081","name":"datagen-purchase1"},"tasks":[],"type":"source"}[root@2869ee7699af appuser]# 

curl -X GET http://schema-registry:8081/subjects/purchase-value/versions/1
curl -X GET http://schema-registry:8081/subjects/product-value/versions/1



curl -X POST -H "Content-Type: application/json" \
    --data @/kafka/jdbc-source-postgres.json \
    http://connect:8083/connectors

{"name":"jdbc_source_postgres","config":{"connector.class":"io.confluent.connect.jdbc.JdbcSourceConnector","connection.url":"jdbc:postgresql://127.0.0.1:5432/plf_training","connection.user":"platformatory","connection.password":"plf_password","kafka.topic":"product","mode":"timestamp","timestamp.column.name":"update_ts","name":"jdbc_source_postgres"},"tasks":[],"type":"source"}

docker-compose exec ksqldb-cli ksql http://ksqldb-server:8088
SHOW TOPICS;

Steps Overview:

    Create a Stream: If you haven’t already, create a stream that will consume the purchase topic.
    Calculate Sales Amount: Create a new calculated field for the total sales amount.
    Create a Windowed Aggregate: Group the data by product and use a window to calculate the total sales amount over the past hour.
    Use a Latest Aggregation: Identify the top-performing product within each window.
    Output in Avro Format: Set the output to be in Avro format.

Let’s break this down:
Step 1: Create a Stream from purchase Topic

If you haven't created a KSQL stream yet for the purchase topic, you'll need to do so:

CREATE STREAM purchase_stream (
  id BIGINT,
  product_id STRING,
  quantity BIGINT,
  customer_id STRING,
  discount DOUBLE,
  price DOUBLE
) WITH (
  KAFKA_TOPIC='purchase',
  VALUE_FORMAT='AVRO',
  PARTITIONS=6
);


Step 2: Calculate Sales Amount

Now, create a computed column for the sales_amount, which is calculated as:

sales_amount = quantity * price * (1 - discount / 100)

You can create a new stream or table to calculate the sales amount for each purchase:

CREATE STREAM sales_stream AS
  SELECT
    product_id,
    quantity,
    price,
    discount,
    (quantity * price * (1 - discount / 100)) AS sales_amount
  FROM purchase_stream
  EMIT CHANGES;

Step 3: Create a Windowed Aggregate

To calculate the total sales amount of each product in a rolling window of 1 hour, you can use a Tumbling Window (or Hopping Window if you want overlap). We’ll aggregate the sales for every 5 minutes and keep the data for the past 1 hour.

CREATE TABLE top_products AS
  SELECT
    product_id,
    SUM(sales_amount) AS total_sales_amount
  FROM sales_stream
  WINDOW TUMBLING (SIZE 1 HOUR)
  GROUP BY product_id
  EMIT CHANGES;
----
1. Create a Stream for Sales Data

Ensure the sales_stream exists first (if not already created):

CREATE STREAM sales_stream (
    product_id STRING,
    sales_amount DOUBLE
) WITH (KAFKA_TOPIC='sales_topic', VALUE_FORMAT='AVRO');

CREATE STREAM sale_stream (
    product_id STRING,
    sales_amount DOUBLE
) WITH (KAFKA_TOPIC='sales_topic', VALUE_FORMAT='AVRO');


2. Create a Stream for Aggregated Sales Data in 1-Hour Windows

You need to use a windowed stream aggregation to get total sales by product every 1 hour. Here's how you can do that:

CREATE STREAM top_products_stream 
  WITH (KAFKA_TOPIC='top_products_stream', PARTITIONS=6, REPLICAS=3) 
  AS SELECT 
    product_id, 
    SUM(sales_amount) AS total_sales_amount 
  FROM sales_stream 
  WINDOW TUMBLING (SIZE 1 HOUR) 
  GROUP BY product_id 
  EMIT CHANGES;

Explanation

    CREATE STREAM is used because you are creating a stream that continuously receives and processes data.
    The aggregation (SUM) is done on the sales_amount, grouped by product_id.
    WINDOW TUMBLING (SIZE 1 HOUR) is a time-based window that will aggregate sales for each product in 1-hour intervals.
    The EMIT CHANGES clause is necessary for continuous query processing in KSQL to push results as they are computed.

3. Using Hopping Window for Top Product Every 5 Minutes

After you successfully create the top_products_stream, you can move forward with the hopping window for calculating the top-performing product every 5 minutes based on sales in the last hour.

CREATE STREAM top_product_stream 
  WITH (KAFKA_TOPIC='top_product_stream', PARTITIONS=6, REPLICAS=3) 
  AS SELECT 
    product_id, 
    total_sales_amount 
  FROM top_products_stream 
  WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES) 
  GROUP BY product_id 
  EMIT CHANGES;

This should now correctly create a stream that calculates the top-performing product for each 5-minute window, based on the aggregated sales from the last 1 hour.

This will calculate the total sales for each product in 1-hour windows.
Step 4: Identify the Top Product Every 5 Minutes

Now, to find the top-performing product (product with the highest total sales) for each window, you can use the LATEST aggregation function to retrieve the latest product information. To get the product with the highest sales, we can use a combination of the RANK function or a similar approach to sort by sales amount.

Here’s how you can do that:

CREATE TABLE top_product_per_5min AS
  SELECT
    product_id,
    total_sales_amount
  FROM top_products
  WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
  GROUP BY total_sales_amount
  EMIT CHANGES;


Step 5: Output in Avro Format

Make sure your output topic is set to use AVRO format for the top product stream. You can configure the stream or table to write out the results in Avro.

CREATE STREAM top_product_stream
  WITH (KAFKA_TOPIC='top_product', VALUE_FORMAT='AVRO') AS
  SELECT * FROM top_product_per_5min EMIT CHANGES;

Summary of Steps:

    Create a stream from the purchase topic.
    Calculate the sales amount (quantity * price * (1 - discount / 100)).
    Use a tumbling window (or hopping window) to aggregate sales by product for the past 1 hour.
    Use the RANK or ORDER BY clause to identify the top product.
    Output the results in Avro format.

By following these steps, you should be able to create a stream that calculates and outputs the top-performing product (with the highest sales) every 5 minutes, based on the past 1 hour of sales data, in Avro format.

Let me know if you need further clarification!


To include logic for the "past hour" in the context of finding the top-performing product every 5 minutes, we can use hopping windows. Hopping windows allow you to aggregate data for overlapping time intervals, such as every 5 minutes for the last hour.
Updated Steps

    Stream Enrichment: Join PURCHASES_STREAM with PRODUCTS_TABLE to enrich the purchase data.

    Calculate Sales Amount: Compute the SALES_AMOUNT for each event.

    Hopping Window Aggregation: Use a HOPPING WINDOW of size 1 hour, advancing every 5 minutes. This ensures that for every 5-minute interval, we aggregate data from the past hour.

    Find the Top Product: For each hopping window, rank products based on their total sales amount and select the top product.

    Output to Avro: Serialize the output to Avro format.

Implementation in KSQL
Step 1: Enriched Sales Stream

CREATE STREAM ENRICHED_SALES WITH (KAFKA_TOPIC='ENRICHED_SALES', VALUE_FORMAT='AVRO') AS 
SELECT
  P.ID AS PURCHASE_ID,
  PR.ID AS PRODUCT_ID,
  PR.NAME AS PRODUCT_NAME,
  PR.CATEGORY AS PRODUCT_CATEGORY,
  PR.PRICE AS PRODUCT_PRICE,
  P.QUANTITY AS QUANTITY,
  P.DISCOUNT AS DISCOUNT,
  ((P.QUANTITY * PR.PRICE) - (((P.QUANTITY * PR.PRICE) * P.DISCOUNT) / 100)) AS SALES_AMOUNT
FROM PURCHASES_STREAM P
INNER JOIN PRODUCTS_TABLE PR 
ON CAST(P.PRODUCT_ID AS INTEGER) = PR.ID
EMIT CHANGES;

Step 2: Aggregate Sales Using Hopping Window

CREATE TABLE PRODUCT_SALES_HOURLY AS 
SELECT 
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  SUM(SALES_AMOUNT) AS TOTAL_SALES_AMOUNT,
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END
FROM ENRICHED_SALES
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;

Step 3: Find Top Product for Each Window

CREATE TABLE TOP_PRODUCT_HOURLY AS 
SELECT * 
FROM (
  SELECT 
    PRODUCT_ID,
    PRODUCT_NAME,
    PRODUCT_CATEGORY,
    TOTAL_SALES_AMOUNT,
    WINDOW_START,
    WINDOW_END,
    RANK() OVER (PARTITION BY WINDOW_START, WINDOW_END ORDER BY TOTAL_SALES_AMOUNT DESC) AS RANK
  FROM PRODUCT_SALES_HOURLY
)
WHERE RANK = 1
EMIT CHANGES;

Step 4: Output in Avro Format

Ensure the TOP_PRODUCT_HOURLY table is serialized in Avro format:

CREATE STREAM TOP_PRODUCT_HOURLY_AVRO WITH (KAFKA_TOPIC='TOP_PRODUCT_HOURLY_AVRO', VALUE_FORMAT='AVRO') AS
SELECT 
  PRODUCT_ID, 
  PRODUCT_NAME, 
  PRODUCT_CATEGORY, 
  TOTAL_SALES_AMOUNT, 
  WINDOW_START, 
  WINDOW_END
FROM TOP_PRODUCT_HOURLY
EMIT CHANGES;

Explanation of Changes

    Hopping Window:
        Window Size: 1 hour.
        Advance Interval: 5 minutes.
        This ensures that at each 5-minute interval, the aggregation includes data from the last hour.

    Ranking:
        RANK() is used to identify the product with the highest sales in each window.

    Avro Format:
        The final output stream is serialized in Avro format for downstream consumers.

Final Output

The TOP_PRODUCT_HOURLY_AVRO stream will provide:

    Product ID, Name, and Category.
    Total Sales Amount.
    Window Start and End Times.

It refreshes every 5 minutes, providing the top-performing product over the past hour. Let me know if you need help with testing or troubleshooting!




===========================================================================================
1)
CREATE STREAM PURCHASE_STREAM (
  ID BIGINT,
  PRODUCT_ID STRING,
  QUANTITY BIGINT,
  CUSTOMER_ID STRING,
  DISCOUNT DOUBLE
) WITH (
  KAFKA_TOPIC = 'purchase',
  VALUE_FORMAT = 'AVRO',
  PARTITIONS=6
);

2)
CREATE TABLE PRODUCT_TABLE (
  ID STRING PRIMARY KEY,
  NAME STRING,
  CATEGORY STRING,
  PRICE DOUBLE
) WITH (
  KAFKA_TOPIC = 'product',
  VALUE_FORMAT = 'AVRO'
);

3)
SHOW STREAMS;
SHOW TABLES;

4)
SELECT * FROM PURCHASE_STREAM EMIT CHANGES;
SELECT * FROM PRODUCT_TABLE EMIT CHANGES;

5)
CREATE STREAM PURCHASE_STREAM_REPARTITIONED 
WITH (KAFKA_TOPIC='PURCHASE_STREAM_REPARTITIONED', PARTITIONS=3, VALUE_FORMAT='
AVRO') AS 
SELECT 
  ID, 
  PRODUCT_ID, 
  QUANTITY, 
  CUSTOMER_ID, 
  DISCOUNT
FROM PURCHASE_STREAM
PARTITION BY PRODUCT_ID;  -- Partition by the key that matches the table

6)
SELECT * FROM PURCHASE_STREAM_REPARTITIONED EMIT CHANGES;

7)
CREATE STREAM PURCHASE_DETAILS AS 
SELECT 
  P.ID AS PURCHASE_ID, 
  P.PRODUCT_ID, 
  P.QUANTITY, 
  P.CUSTOMER_ID, 
  P.DISCOUNT, 
  PT.NAME AS PRODUCT_NAME, 
  PT.CATEGORY AS PRODUCT_CATEGORY, 
  PT.PRICE AS PRODUCT_PRICE,
  ((P.QUANTITY * PT.PRICE) - (((P.QUANTITY * PT.PRICE) * P.DISCOUNT) / 100)) AS SALES_AMOUNT
FROM PURCHASE_STREAM_REPARTITIONED P
LEFT JOIN PRODUCT_TABLE PT
ON P.PRODUCT_ID = PT.ID
EMIT CHANGES;

8)
SELECT * FROM PURCHASE_DETAILS EMIT CHANGES;


7)
CREATE TABLE PRODUCT_SALES_HOURLY
WITH (KAFKA_TOPIC='PRODUCT_SALES_HOURLY', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  SUM(SALES_AMOUNT) AS TOTAL_SALES_AMOUNT,
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END
FROM PURCHASE_DETAILS
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;

8)
SELECT * FROM PRODUCT_SALES_HOURLY EMIT CHANGES;

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

CREATE TABLE PRODUCT_SALES_HOURLY
WITH (KAFKA_TOPIC='PRODUCT_SALES_HOURLY', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT 
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  SUM(SALES_AMOUNT) AS TOTAL_SALES_AMOUNT,
  MAX(SALES_AMOUNT) AS MAX_SALES_AMOUNT,
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END
FROM PURCHASE_DETAILS
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;

CREATE TABLE PRODUCT_SALES_HOURLY
WITH (KAFKA_TOPIC='PRODUCT_SALES_HOURLY', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT 
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  MAX(SALES_AMOUNT) AS MAX_SALES_AMOUNT,
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END
FROM PURCHASE_DETAILS
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;







CREATE TABLE PRODUCT_SALES_HOURLY
WITH (KAFKA_TOPIC='PRODUCT_SALES_HOURLY', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  MAX(SALES_AMOUNT) AS MAX_SALES_AMOUNT,
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END
FROM PURCHASE_DETAILS
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;


CREATE TABLE PRODUCT_SALES_HOURLY11
WITH (KAFKA_TOPIC='PRODUCT_SALES_HOURLY', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  SALES_AMOUNT AS MAX_SALES_AMOUNT,
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END
FROM PURCHASE_DETAILS
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;
















CREATE TABLE TOP_PRODUCT
WITH (KAFKA_TOPIC='TOP_PRODUCT', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS
SELECT
  MAX(SALES_AMOUNT) AS MAX_PRODUCT,
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END
FROM PURCHASE_DETAILS
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY WINDOWSTART, WINDOWEND
EMIT CHANGES;

CREATE TABLE PRODUCT_SALES_AGGREGATE
WITH (KAFKA_TOPIC='PRODUCT_SALES_AGGREGATE', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  SUM(SALES_AMOUNT) AS TOTAL_SALES_AMOUNT
FROM PURCHASE_DETAILS
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;

SELECT * FROM PRODUCT_SALES_AGGREGATE EMIT CHANGES;


CREATE TABLE TOP_PRODUCT
WITH (KAFKA_TOPIC='TOP_PRODUCT', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  MAX(TOTAL_SALES_AMOUNT) AS MAX_TOTAL_SALES_AMOUNT,
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END
FROM PRODUCT_SALES_AGGREGATE
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;


CREATE STREAM PRODUCT_SALES_AGGREGATES
WITH (KAFKA_TOPIC='PRODUCT_SALES_AGGREGATES', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  SUM(SALES_AMOUNT) AS TOTAL_SALES_AMOUNT
FROM PURCHASE_DETAILS
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;

CREATE TABLE PRODUCT_SALES_HOURLY
WITH (KAFKA_TOPIC='PRODUCT_SALES_HOURLY', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  MAX(TOTAL_SALES_AMOUNT),
  WINDOW_START,
  WINDOW_END
FROM (
SELECT
    PRODUCT_ID,
    PRODUCT_NAME,
    PRODUCT_CATEGORY,
    SUM(SALES_AMOUNT) AS TOTAL_SALES_AMOUNT,
    WINDOWSTART AS WINDOW_START,
    WINDOWEND AS WINDOW_END
  FROM PURCHASE_DETAILS
  WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
  GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
)
GROUP BY WINDOW_START, WINDOW_END, PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;

SELECT
  PRODUCTID,
  PRODUCTNAME,
  PRODUCTCATEGORY,
  MAX(TOTALSALESAMOUNT) AS MAXTOTALSALESAMOUNT,
  WINDOWSTART,
  WINDOWEND
FROM (
  SELECT
    PRODUCTID,
    PRODUCTNAME,
    PRODUCTCATEGORY,
    SUM(SALESAMOUNT) AS TOTALSALESAMOUNT,
    WINDOWSTART,
    WINDOWEND
  FROM your_table_name
  GROUP BY PRODUCTID, PRODUCTNAME, PRODUCTCATEGORY, WINDOWSTART, WINDOWEND
) AS subquery
GROUP BY PRODUCTID, PRODUCTNAME, PRODUCTCATEGORY, WINDOWSTART, WINDOWEND;


CREATE TABLE TOP_PRODUCT AS
SELECT
  WINDOW_START,
  WINDOW_END,
  MAX(total_sales) AS max_sales_amount,
  product_id,
  customer_id
FROM (
  SELECT
    WINDOWSTART AS WINDOW_START,
    WINDOWEND AS WINDOW_END,
    SUM(sales_amount) AS total_sales,
    p.product_id,
    p.customer_id
  FROM PURCHASESTREAMREPARTITIONED AS ps
  WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
  JOIN PRODUCTTABLE AS p ON ps.product_id = p.id
  GROUP BY
    TUMBLE($rowtime, INTERVAL '1' HOUR),
    p.product_id,
    p.customer_id
)
GROUP BY window_start, window_end, product_id, customer_id
HAVING window_start = window_end - INTERVAL '1' HOUR;

CREATE TABLE TOP_PRODUCT AS
SELECT
  window_start,
  window_end,
  MAX(total_sales) AS max_sales_amount,
  product_id,
  customer_id
FROM (
  SELECT
    TUMBLE_START($rowtime, INTERVAL '1' HOUR) AS window_start,
    TUMBLE_END($rowtime, INTERVAL '1' HOUR) AS window_end,
    SUM(sales_amount) AS total_sales,
    p.product_id,
    p.customer_id
  FROM PURCHASESTREAMREPARTITIONED AS ps
  JOIN PRODUCTTABLE AS p ON ps.product_id = p.id
  GROUP BY
    TUMBLE($rowtime, INTERVAL '1' HOUR),
    p.product_id,
    p.customer_id
)
GROUP BY window_start, window_end, product_id, customer_id
HAVING window_start = window_end - INTERVAL '1' HOUR;



SELECT
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  MAX(TOTAL_SALES_AMOUNT) AS MAX_SALES_AMOUNT
FROM PRODUCT_SALES_HOURLY
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;


CREATE STREAM TOP_PERFORMING_PRODUCT_STREAM 
WITH (KAFKA_TOPIC='TOP_PERFORMING_PRODUCT', PARTITIONS=3, REPLICAS=3, VALUE_FORMAT='AVRO') AS
SELECT
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  TOTAL_SALES_AMOUNT
FROM PRODUCT_SALES_HOURLY
EMIT CHANGES;


CREATE STREAM TOTAL_SALES_DETAILS
WITH (KAFKA_TOPIC='TOTAL_SALES_DETAILS', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT
  PD.PURCHASE_ID AS PURCHASE_ID,
  PD.PRODUCT_ID,
  PD.QUANTITY,
  PD.CUSTOMER_ID,
  PD.DISCOUNT,
  PD.PRODUCT_NAME,
  PD.PRODUCT_CATEGORY,
  PD.PRODUCT_PRICE,
  SUM(PD.SALES_AMOUNT) AS TOTAL_SALES_AMOUNT
FROM PURCHASE_DETAILS PD
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY PURCHASE_ID, PRODUCT_ID, QUANTITY, CUSTOMER_ID, DISCOUNT, PRODUCT_NAME, PRODUCT_CATEGORY, PRODUCT_PRICE
EMIT CHANGES;

SELECT * FROM TOTAL_SALES_DETAILS EMIT CHANGES;


CREATE TABLE PRODUCT_SALES_HOURLY
WITH (KAFKA_TOPIC='PRODUCT_SALES_HOURLY10', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT
  PD.PURCHASE_ID AS PURCHASE_ID,
  PD.PRODUCT_ID,
  PD.QUANTITY,
  PD.CUSTOMER_ID,
  PD.DISCOUNT,
  PD.PRODUCT_NAME,
  PD.PRODUCT_CATEGORY,
  PD.PRODUCT_PRICE,
  MAX(PD.TOTAL_SALES_AMOUNT),
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END
FROM TOTAL_SALES_DETAILS PD
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY PURCHASE_ID, PRODUCT_ID, QUANTITY, CUSTOMER_ID, DISCOUNT, PRODUCT_NAME, PRODUCT_CATEGORY, PRODUCT_PRICE
EMIT CHANGES;

CREATE STREAM TOTAL_SALES_DETAILS3 AS
SELECT
  PD.PURCHASE_ID AS PURCHASE_ID,
  PD.PRODUCT_ID,
  PD.QUANTITY,
  PD.CUSTOMER_ID,
  PD.DISCOUNT,
  PD.PRODUCT_NAME,
  PD.PRODUCT_CATEGORY,
  PD.PRODUCT_PRICE,
  SUM(PD.SALES_AMOUNT)
FROM PURCHASE_DETAILS PD
GROUP BY PURCHASE_ID, PRODUCT_ID, QUANTITY, CUSTOMER_ID, DISCOUNT, PRODUCT_NAME, PRODUCT_CATEGORY, PRODUCT_PRICE
EMIT CHANGES;


MAX(SUM(SALES_AMOUNT))

SELECT 
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  MAX(TOTAL_SALES_AMOUNT) AS MAX_SALES_AMOUNT,
  WINDOW_START,
  WINDOW_END
FROM PRODUCT_SALES_HOURLY
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY, WINDOW_START, WINDOW_END
EMIT CHANGES;







SELECT 
    WINDOW_START, 
    WINDOW_END, 
    PRODUCT_ID, 
    PRODUCT_NAME, 
    PRODUCT_CATEGORY, 
    MAX(TOTALSALESAMOUNT) AS MAXSALESAMOUNT
FROM 
    TABLE(
        TUMBLE(TABLE PRODUCT_SALES_HOURLY, DESCRIPTOR($rowtime), INTERVAL '1' HOUR)
    )
GROUP BY 
    WINDOW_START, 
    WINDOW_END, 
    PRODUCT_ID, 
    PRODUCT_NAME, 
    PRODUCT_CATEGORY;


CREATE TABLE PRODUCT_SALES_HOURLY2
WITH (KAFKA_TOPIC='PRODUCT_SALES_HOURLY2', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT 
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  SUM(SALES_AMOUNT) AS TOTAL_SALES_AMOUNT,
  MAX(SALES_AMOUNT) AS MAX_SALES_AMOUNT,
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END
FROM PURCHASE_DETAILS
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;

CREATE STREAM PRODUCT_SALES_HOURLY3
WITH (KAFKA_TOPIC='PRODUCT_SALES_HOURLY3', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT 
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  SUM(SALES_AMOUNT) AS TOTAL_SALES_AMOUNT,
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END
FROM PURCHASE_DETAILS
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;

SELECT * FROM PRODUCT_SALES_HOURLY2 EMIT CHANGES;


SELECT 
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  MAX(TOTAL_SALES_AMOUNT) AS MAX_SALES_AMOUNT,
  WINDOW_START,
  WINDOW_END
FROM PRODUCT_SALES_HOURLY
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY, WINDOW_START, WINDOW_END
EMIT CHANGES;




CREATE TABLE TOP_PRODUCT
WITH (KAFKA_TOPIC='TOP_PRODUCT', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT 
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  MAX(SALES_AMOUNT) AS TOP_PRODUCT,
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END
FROM PURCHASE_DETAILS
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;

SELECT * FROM TOP_PRODUCT EMIT CHANGES;

CREATE TABLE TOP_PERFORMING_PRODUCT AS
SELECT
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  MAX(TOTAL_SALES_AMOUNT) AS TOP_SALES_AMOUNT,
  WINDOW_START,
  WINDOW_END
FROM PRODUCT_SALES_HOURLY
GROUP BY WINDOW_START, WINDOW_END
EMIT CHANGES;


CREATE TABLE TOP_PRODUCT_EVERY_5_MINUTES WITH (
  KAFKA_TOPIC='TOP_PRODUCT_5_MIN',
  KEY_FORMAT='AVRO',
  VALUE_FORMAT='AVRO',
  PARTITIONS=3,
  REPLICAS=3
) AS
SELECT * 
FROM (
  SELECT 
    PRODUCT_ID,
    PRODUCT_NAME,
    PRODUCT_CATEGORY,
    TOTAL_SALES_AMOUNT,
    WINDOW_START,
    WINDOW_END,
    ROW_NUMBER() OVER (
      PARTITION BY WINDOWSTART, WINDOWEND 
      ORDER BY TOTAL_SALES_AMOUNT DESC
    ) AS RANK
  FROM PRODUCT_SALES_HOURLY
)
WHERE RANK = 1
EMIT CHANGES;


CREATE TABLE PRODUCT_SALES_HOURLY2
WITH (KAFKA_TOPIC='PRODUCT_SALES_HOURLY2', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS 
SELECT 
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  SUM(SALES_AMOUNT) AS TOTAL_SALES_AMOUNT,
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END
FROM PURCHASE_DETAILS
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY SALES_AMOUNT
EMIT CHANGES;

SELECT TOTAL_SALES_AMOUNT
FROM PRODUCT_SALES_HOURLY
GROUP BY TOTAL_SALES_AMOUNT
EMIT CHANGES
LIMIT 2;

SELECT RELEASE_YEAR,
       MIN(TOTAL_SALES) AS MIN__TOTAL_SALES,
       MAX(TOTAL_SALES) AS MAX__TOTAL_SALES
FROM MOVIE_SALES
GROUP BY RELEASE_YEAR
EMIT CHANGES
LIMIT 2;

SELECT *
FROM (
  SELECT *,
    ROW_NUMBER() OVER (ORDER BY TOTAL_SALES_AMOUNT DESC) AS row_num
  FROM PRODUCT_SALES_HOURLY
)
WHERE row_num = 1;  -- For the top product overall


SELECT
    PRODUCT_ID,
    PRODUCT_NAME,
    PRODUCT_CATEGORY,
    TOTAL_SALES_AMOUNT
FROM
    PRODUCT_SALES_HOURLY
ORDER  TOTAL_SALES_AMOUNT
LIMIT 1;




CREATE TABLE MAX_SALES_PER_WINDOW AS 
SELECT 
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END,
  MAX(TOTAL_SALES_AMOUNT) AS MAX_SALES_AMOUNT
FROM PRODUCT_SALES_HOURLY
GROUP BY WINDOWSTART, WINDOWEND
EMIT CHANGES;



CREATE TABLE PRODUCT_SALES_HOURLY_AGGREGATED AS 
SELECT 
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  TOTAL_SALES_AMOUNT,
  WINDOW_START,
  WINDOW_END
FROM PRODUCT_SALES_HOURLY
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY, WINDOW_START, WINDOW_END
EMIT CHANGES;


You can split the operation into two steps:

    Create an aggregate stream or table that calculates the total sales per product per window.
    Then, query that table to find the maximum sales amount per product over the time window.

Let’s break it down.
Step 1: Aggregate Sales by Product in a Windowed Table

You need to aggregate the sales in a hopping window. Create a windowed table that will sum the sales for each product, per window:

CREATE TABLE PRODUCT_SALES_HOURLY
WITH (KAFKA_TOPIC='PRODUCT_SALES_HOURLY', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS
SELECT
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  SUM(SALES_AMOUNT) AS TOTAL_SALES_AMOUNT,
  WINDOWSTART AS WINDOW_START,
  WINDOWEND AS WINDOW_END
FROM PURCHASE_DETAILS
WINDOW HOPPING (SIZE 1 HOURS, ADVANCE BY 5 MINUTES)
GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;

Step 2: Query the Maximum Sales Amount Per Product

After you have the aggregated sales in the PRODUCT_SALES_HOURLY table, you can query it to find the top-performing product. Use MAX() here to get the product with the maximum sales in a particular time window:

CREATE TABLE TOP_PERFORMING_PRODUCT
WITH (KAFKA_TOPIC='TOP_PERFORMING_PRODUCT', VALUE_FORMAT='AVRO', KEY_FORMAT='AVRO') AS
SELECT
  PRODUCT_ID,
  PRODUCT_NAME,
  PRODUCT_CATEGORY,
  MAX(TOTAL_SALES_AMOUNT) AS TOP_SALES_AMOUNT,
  WINDOW_START,
  WINDOW_END
FROM PRODUCT_SALES_HOURLY
GROUP BY WINDOW_START, WINDOW_END, PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY
EMIT CHANGES;

Why this works:

    Step 1: In this query, you use the windowing function to aggregate the sales for each product in a 1-hour window, advancing every 5 minutes.
    Step 2: In the second query, you aggregate the results from the first table (PRODUCT_SALES_HOURLY) to find the top-performing product by calculating the maximum total sales amount.

==
CREATE TABLE PRODUCT_SALES_HOURLY AS SELECT PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY, SUM(SALES_AMOUNT) AS TOTAL_SALES_AMOUNT, WINDOWSTART AS WINDOW_START, WINDOWEND AS WINDOW_END FROM PURCHASE_DETAILS WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES) GROUP BY PRODUCT_ID, PRODUCT_NAME, PRODUCT_CATEGORY EMIT CHANGES; After doing this, calculate max of TOTAL_SALES_AMOUNT to get top performing product 
==